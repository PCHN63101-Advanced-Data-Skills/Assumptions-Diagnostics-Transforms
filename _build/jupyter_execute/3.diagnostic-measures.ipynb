{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6b3fd6-1753-45e7-a920-fccf68cbdcac",
   "metadata": {},
   "source": [
    "# Diagnostic Measures\n",
    "Now that we have established both the core assumptions of the linear model and have discussed some important data features, we can now examine some of the key diagnostic measures we will use for assessing both of these elements. Importantly, these diagnostics are all calculated directly from the model itself. As such, we do not usually check the assumptions *before* fitting the model. Instead, we fit the model, derive diagnostic measures from the model and then check its suitability. This is important because we want our diagnostics to be tied as closely to the model as possible. Although possible to create useful visualisations from the raw data, we can only accurately assess the model if we have the precise numeric values the model is using. This necessitates fitting the model *first*. However, we need to take care that we do not enter into *interpreting* the model until we are satisfied the assumptions are suitably met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18769fa6",
   "metadata": {},
   "source": [
    "## Leverage Values\n",
    "We will start our tour of diagnostic measures with the *leverage values* because, as we will see below, these have some important consequences for interpreting other diagnostic measures. As mentioned above, leverage concerns the degree to which a single data point is influencing the model fit. The *leverage values* are measures of leverage and range $0 \\leq h_{i} \\leq 1$. These can be interpreted as\n",
    "\n",
    "- If $h_{i}$ is close to 0, then the predicted value $\\hat{y}_{i}$ is mostly determined by the other data points.\n",
    "- If $h_{i}$ is close to 1, then the predicted value $\\hat{y}_{i}$ is determined almost entirely by $y_{i}$. \n",
    "\n",
    "So, in this sense, higher leverage means that the predicted value of point $i$ is not very related to the rest of the data in the dataset, as it depends almost entirely on that single point. This implies that this point is an outlier in predictor space, but also that the model fit for that combination of predictor values is not a balance between mutliple observations, as it is biased towards that single observation.\n",
    "\n",
    "The actual calculation of the leverage values in multiple regression is complex and involves diving into the linear algebraic representation of linear models (which is beyond the scope of the unit). However, they can be easily extracted in `R` using the `hatvalues()` function[^hatmat-foot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7634065",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n",
      "         0.07495754          0.05815750          0.08563345          0.05334688 \n",
      "  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n",
      "         0.10981841          0.06986080          0.11762078          0.15741830 \n",
      "           Merc 230            Merc 280           Merc 280C          Merc 450SE \n",
      "         0.15201954          0.04691578          0.04691578          0.07862927 \n",
      "         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n",
      "         0.08370579          0.08169851          0.20155034          0.24373270 \n",
      "  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n",
      "         0.23547715          0.08274176          0.13078212          0.09961207 \n",
      "      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n",
      "         0.09155927          0.14918089          0.15654328          0.10338967 \n",
      "   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n",
      "         0.08575683          0.09233897          0.08597706          0.16171087 \n",
      "     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n",
      "         0.20532691          0.07037046          0.46356582          0.12368549 \n"
     ]
    }
   ],
   "source": [
    "data(mtcars)\n",
    "mod <- lm(mpg ~ wt + hp + cyl, data=mtcars)\n",
    "lev <- hatvalues(mod)\n",
    "print(lev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daefbdd",
   "metadata": {},
   "source": [
    "None of the data here exerts very large leverage in *absolute* terms, as all the leverage values are quite far below 1. Neverthless, it is typical to interpret leverage in a *relative* fashion for a given dataset. As such, a common cutoff for defining *high leverage* is $h_{i} > 2\\frac{p}{n}$, where $p$ is the number of model parameters and $n$ is the sample size. For reasons related to how leverage is calculated, this gives a cutoff of *twice* the average leverage[^levcutoff-foot]. For the `mtcars` model above this would be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae62256",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.25\n"
     ]
    }
   ],
   "source": [
    "n       <- length(mod$fitted.values)\n",
    "p       <- length(mod$coefficients)\n",
    "big.lev <- 2*p/n\n",
    "print(big.lev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1eb39c",
   "metadata": {},
   "source": [
    "So our *relative* cutoff would be $h_{i} > 0.25$. We can examine any concerning cases using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1062f4c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maserati Bora \n",
      "    0.4635658 \n"
     ]
    }
   ],
   "source": [
    "print(lev[lev > big.lev])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2529f0bd",
   "metadata": {},
   "source": [
    "So we have one observation with high relative leverage, suggesting this may be an outlier in predictor space and may be unduly influencing the model fit. We would need to examine this specific case in more detail and assess why its particular combination of predictor values is unusual, and whether any mistakes have been made. We could also assess the sensitivity of our model to this point by fitting it both *with* and *without* this observation, taking note of whether our inference substantially changes. We will see how leverage factors into some standard diagnostic plots a little later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4044bc7a",
   "metadata": {},
   "source": [
    "## Residuals\n",
    "Beyond leverage values, one of the most useful diagnostic we can get from our model is the *residuals*. As we know, many of the linear model assumptions centre on the distribution of the errors. As such, it would stand to reason that we can use the residuals as a proxy for the errors to assess these assumptions (though there is a catch, as we will discuss below). Beyond the distributional assumptions, we can also use the residuals to detect outliers, check for evidence of non-linearity in the model fit, and can diagnose any issues with the assumption of a continuous outcome when working with data that is technically non-continuous. So, as we can see, the residuals are incredibly useful as a diagnostic tool and can be extracted using the `resid()` function in `R`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c26655d0",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n",
      "         -1.8204257          -1.0128476          -3.1603990           0.4639233 \n",
      "  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n",
      "          1.5322025          -2.1503588          -1.1934238           0.6356864 \n",
      "           Merc 230            Merc 280           Merc 280C          Merc 450SE \n",
      "         -0.4957351          -0.7890124          -2.1890124           1.3175861 \n",
      "         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n",
      "          1.1408152          -0.8008361          -0.4944331           0.2370012 \n",
      "  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n",
      "          4.5573819           5.5725355           1.4673228           5.8985522 \n",
      "      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n",
      "         -3.9290355          -1.8653922          -2.4345849          -1.3383411 \n",
      "   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n",
      "          3.3148266          -0.3667124          -0.5665304           2.2446157 \n",
      "     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n",
      "         -0.6174892          -1.4729031           1.1300053          -2.8149817 \n"
     ]
    }
   ],
   "source": [
    "resid.raw <- resid(mod)\n",
    "print(resid.raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a567ed84",
   "metadata": {},
   "source": [
    "For each car, the residual value represents the difference between the model prediction and the original data. The residuals are therefore in the original units of the outcome which, in this example, is MPG. This can be difficult to interpret in terms of defining which residuals are *large*, but this is something we will come back to below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631ffd09",
   "metadata": {},
   "source": [
    "### Some Complexities with Raw Residuals\n",
    "As indicated above, it would seem reasonable to assume that we can use the residuals as a proxy for the true errors and thus use them to assess the distributional assumptions of our model. Although we can do this in an *approximate* fashion, there is an important technical detail to consider. One of the main reasons that we have made a point of distinguishing *errors* from *residuals* is that the estimation process *changes the distributional properties of the errors*. This means that *errors* and *residuals* are not expected to behave identically. So while it is correct to assume\n",
    "\n",
    "$$\n",
    "\\epsilon_{i} \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}\\left(0,\\sigma^{2}\\right),\n",
    "$$\n",
    "\n",
    "it is *not* technically correct to assume the same for the *residuals*. In fact, the variance of the residuals is *not* $\\sigma^{2}$, as we might expect, it is\n",
    "\n",
    "$$\n",
    "\\text{Var}\\left(e_{i}\\right) = \\sigma^{2}\\left(1 - h_{i}\\right).\n",
    "$$\n",
    "\n",
    "So, rather than being constant (as is the case with the true *errors*), the variance of the residuals is *scaled* by the leverage. Because we have a leverage value for each observation, the variance will be different for each observation. In other words, the residuals *cannot be homoscedastic*, even if the errors are. In the equation above, the key element is that the variance is scaled by $\\left(1 - h_{i}\\right)$. So, if $h_{i} = 0.8$ then the variance would be multiplied by $1 - 0.8 = 0.2$. As such, the *higher* the leverage, the *smaller* the variance gets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eab07db",
   "metadata": {},
   "source": [
    "```{admonition} Why Does Leverage Reduce the Variance?\n",
    ":class: tip\n",
    "The reason that this relationship between leverage and variance exists is because of the inherent properties of the estimation process. This is easiest to think about in terms of *least-squares*. In order to minimise the sum of squared residuals, observations with high leverage pull the model fit towards them. Because there may be no other points around them, there is nothing to balance the model fit by pulling in the opposite direction. The result is that the variance around those points will be smaller than elsewhere in the data, where the fit is more of a balanced between many data points. As such, points with high leverage will often sit closer to the regression line, making the variance *smaller* for that particular combination of predictor values. The question is then whether the influence of these individual data points is biasing the model, despite initial appearences of a good fit.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd2acfc",
   "metadata": {},
   "source": [
    "### Standardised Residuals\n",
    "At this point, we have established that residuals are potentially very useful diagnostically, but they have two issues:\n",
    "\n",
    "1. Residuals are in the original units of the outcome and therefore difficult to interpret in terms of which are *large* and which are *small*.\n",
    "2. The homoscedasticity of the residuals depends upon the leverage, even if the true errors are homoscedastic. So any assessment of constant variance that uses the residuals could be misleading.\n",
    "\n",
    "In order to solve both these issues, we can *standardise* the residuals by computing \n",
    "\n",
    "$$\n",
    "r_{i} = \\frac{e_{i}}{\\hat{\\sigma}\\sqrt{1 - h_{i}}},\n",
    "$$\n",
    "\n",
    "which is just the residual divided by its theoretical standard deviation. This has the effect of scaling the residuals into units of standard deviation. So, for the purpose of outlier detection, we just need to decide how many standard deviations we would consider *large*. Typically, values of either 2 or 3 standard deviations are used to detect outliers. This scaling also has an additional advantage because the effect of leverage is removed from each residual. This makes standardised residuals useful for decting outliers *and* for assessing homoscedasticity, as the misleading effect of leverage is gone. Standardised residuals can be calculated in `R` using the `rstandard()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3dc2dd1",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n",
      "         -0.7536168          -0.4155405          -1.3159524           0.1898494 \n",
      "  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n",
      "          0.6465994          -0.8877596          -0.5058544           0.2757372 \n",
      "           Merc 230            Merc 280           Merc 280C          Merc 450SE \n",
      "         -0.2143459          -0.3217930          -0.8927730           0.5465378 \n",
      "         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n",
      "          0.4745219          -0.3327434          -0.2203141           0.1085104 \n",
      "  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n",
      "          2.0752892           2.3166769           0.6266424           2.4750788 \n",
      "      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n",
      "         -1.6413307          -0.8052115          -1.0554848          -0.5627602 \n",
      "   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n",
      "          1.3803474          -0.1532577          -0.2359408           0.9761207 \n",
      "     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n",
      "         -0.2757995          -0.6082438           0.6143003          -1.1973029 \n"
     ]
    }
   ],
   "source": [
    "data(mtcars)\n",
    "mod       <- lm(mpg ~ wt + hp + cyl, data=mtcars)\n",
    "resid.std <- rstandard(mod)\n",
    "print(resid.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b89f85",
   "metadata": {},
   "source": [
    "Here we can see a few observations with residuals larger than 2 standard deviations from the model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e4d4537",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chrysler Imperial          Fiat 128    Toyota Corolla \n",
      "         2.075289          2.316677          2.475079 \n"
     ]
    }
   ],
   "source": [
    "print(resid.std[abs(resid.std) > 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b35b2",
   "metadata": {},
   "source": [
    "These could be potential outliers, though we will see ways of visualising these later so that they can be seen in context with the rest of the data. We will also see how visualisations can be used to assess homoscedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bd613f",
   "metadata": {},
   "source": [
    "### Studentised Residuals\n",
    "An alternative, but potentially more powerful approach, to standardised residuals is to computer *studentised* residuals. These are based on scaling the residuals into standard deviation units, but they do so differently for each individual datapoint. Rather than using the global variance estimate $\\hat{\\sigma}^{2}$, studentised residuals use $\\hat{\\sigma}^{2}_{-i}$, which is the variance estimate from a model with datapoint $i$ removed. \n",
    "\n",
    "$$\n",
    "t_{i} = \\frac{e_{i}}{\\hat{\\sigma}_{-i}\\sqrt{1 - h_{i}}},\n",
    "$$\n",
    "\n",
    "The reason for doing this is that an outlier will artifically inflate the global variance estimate, meaning that extreme points will have *smaller* values when standardised. In other words, outliers can *hide* and appear less extreme than they really are. This is corrected by removing the point in question from the variance when scaling it into standard deviation units. Studentised residuals can be calculated using the `rstudent()` function in `R`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "444cc1ba",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n",
      "         -0.7476584          -0.4093168          -1.3341552           0.1865485 \n",
      "  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n",
      "          0.6397422          -0.8842967          -0.4990246           0.2711369 \n",
      "           Merc 230            Merc 280           Merc 280C          Merc 450SE \n",
      "         -0.2106564          -0.3165804          -0.8894363           0.5395753 \n",
      "         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n",
      "          0.4678563          -0.3273955          -0.2165319           0.1065775 \n",
      "  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n",
      "          2.2153833           2.5303244           0.6197115           2.7498370 \n",
      "      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n",
      "         -1.6953756          -0.8000188          -1.0577210          -0.5557716 \n",
      "   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n",
      "          1.4040889          -0.1505592          -0.2319199           0.9752688 \n",
      "     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n",
      "         -0.2711984          -0.6012690           0.6073374          -1.2070326 \n"
     ]
    }
   ],
   "source": [
    "resid.t <- rstudent(mod)\n",
    "print(resid.t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c26447",
   "metadata": {},
   "source": [
    "Here we can see that the previous standardised residuals with values over 2 have all gotten *bigger* when using studentised residuals. This shows how their inclusion was artifically inflating the variance and thus making their standardised score *smaller*. This makes studentised residuals much more useful for detecting outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b15a9275",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chrysler Imperial          Fiat 128    Toyota Corolla \n",
      "         2.215383          2.530324          2.749837 \n"
     ]
    }
   ],
   "source": [
    "print(resid.t[abs(resid.t) > 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957e8650",
   "metadata": {},
   "source": [
    "It is also interesting to note that the `Maserati Bora`, which had high relative leverage, has *not* been identified as an outlier. This highlights how the two concepts are somewhat independent, particularly when we recognise that observations with high leverage will often live close to the regression line and thus will have *smaller* residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55051126",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "```{admonition} The Distribution of Studentised Residuals\n",
    ":class: tip\n",
    "Another advantage of *studentised* residuals is that they have a known distribution. As the notation $t_{i}$ implies, the residuals are distributed as a $t$-distribution with $n-p-1$ degrees of freedom. This means that we *could* calculate a $p$-value for each residual to test the null hypothesis that this observation is consistent with the model. For instance, we can calculate `2 * pt(abs(resid.t), df=mod$df.residual-1, lower.tail=FALSE)` to get $p$-values for the studentised residuals. *However*, we have calculated $n$ tests here and so would need some form of correction (which we will discuss further next week). Furthermore, we are again relying on NHST here for assumption checking, which we have already indicated is flawed. However, we will see how this distribution crops up again a little later on. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508af2f",
   "metadata": {},
   "source": [
    "## Predicted Values\n",
    "Beyond the residuals and their standardisation, the *predicted* values from the model are very useful diagnostically. As a reminder, the predicted values in a regression model are\n",
    "\n",
    "$$\n",
    "\\hat{y}_{i} = E(y_{i}|\\mathbf{x}_{i}) = \\hat{\\beta}_{0} + \\sum_{j=1}^{p} \\hat{\\beta}_{j}x_{ij},\n",
    "$$\n",
    "\n",
    "which are the results of evaluating the model equation for each observation. These are also known as the *fitted* values, and can be extracted using the `fitted()` function in `R`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "217cb119",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n",
      "           22.82043            22.01285            25.96040            20.93608 \n",
      "  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n",
      "           17.16780            20.25036            15.49342            23.76431 \n",
      "           Merc 230            Merc 280           Merc 280C          Merc 450SE \n",
      "           23.29574            19.98901            19.98901            15.08241 \n",
      "         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n",
      "           16.15918            16.00084            10.89443            10.16300 \n",
      "  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n",
      "           10.14262            26.82746            28.93268            28.00145 \n",
      "      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n",
      "           25.42904            17.36539            17.63458            14.63834 \n",
      "   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n",
      "           15.88517            27.66671            26.56653            28.15538 \n",
      "     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n",
      "           16.41749            21.17290            13.86999            24.21498 \n"
     ]
    }
   ],
   "source": [
    "mod.fit <- fitted(mod)\n",
    "print(mod.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2bdd76",
   "metadata": {},
   "source": [
    "The reason why the prediction is useful diagnostically is because our assumptions about homoscedasticity relate to an equal spread of data above and below the regression plane. Each predicted value represents a point on this plane from some combination of predictor values. As such, when we plot the predicted values against the standardised residuals, we should see an even and equal vertical spread of points above and below the predictions. We can also use this form of visualisation to assess evidence of *non-linear* relationships in the data. For instance, if we have a straight-line fit but the actual relationship is a *curve*, the residuals will not be evenly spread around the prediction and will have some characteristic shape that bends around the prediction. We will see this when we examine some standard diagnostic plots in the next part of the lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618eee05",
   "metadata": {},
   "source": [
    "## Cook's Distance\n",
    "Another common diagnostic measure is a quantity known as *Cook's Distance*, which combines both the studentised residuals and the leverage to produce a single metric of whether an observation is an outlier in *both* outcome and predictor space. For each observation, the Cook's Distance $D_{i}$ is calculated as\n",
    "\n",
    "$$\n",
    "D_{i} = \\frac{1}{p} \\times t_{i}^{2} \\times \\frac{h_{i}}{1 - h_{i}},\n",
    "$$\n",
    "\n",
    "These values can be returned in `R` using the `cooks.distance()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa8664b0",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n",
      "       0.0115052172        0.0026655956        0.0405455760        0.0005077811 \n",
      "  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n",
      "       0.0128945786        0.0147984549        0.0085274628        0.0035511903 \n",
      "           Merc 230            Merc 280           Merc 280C          Merc 450SE \n",
      "       0.0020591309        0.0012743271        0.0098086603        0.0063727998 \n",
      "         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n",
      "       0.0051424860        0.0024625653        0.0030630946        0.0009486833 \n",
      "  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n",
      "       0.3316313326        0.1210330843        0.0147706379        0.1694339333 \n",
      "      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n",
      "       0.0678793903        0.0284207738        0.0516910757        0.0091297778 \n",
      "   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n",
      "       0.0446810928        0.0005973732        0.0013090965        0.0459507317 \n",
      "     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n",
      "       0.0049134259        0.0070012535        0.0815260489        0.0505832576 \n"
     ]
    }
   ],
   "source": [
    "D <- cooks.distance(mod)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc135601",
   "metadata": {},
   "source": [
    "In terms of interpretation, values of $D_{i} > 0.5$ are cause for concern and values of $D_{i} > 1$ are indications of a potential problem. Remember, this relates to a data point being extreme relative to the model fit *and* in terms of the patterns of predictors. This does not necessarily make it *wrong*, but does require investigation. We can filter the returned values of $D_{i}$ to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ace569e8",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "named numeric(0)\n",
      "named numeric(0)\n"
     ]
    }
   ],
   "source": [
    "print(D[D > 0.5])\n",
    "print(D[D > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75835403",
   "metadata": {},
   "source": [
    "In this case, we have *no* observations with high values of $D_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af807aa",
   "metadata": {},
   "source": [
    "## The Variance Inflation Factor (VIF)\n",
    "Our final diagnostic measure deviates from the other measures discussed in this section because it is only a single number, is not a direct output from the model and requires an external package to calculate. Neverthless, this is one of the most useful tools because it helps diagnose issues of *multicollinearity*. In the previous section, we discussed multicollinearity in terms of high correlation between predictors. As a diagnostic measure, we can certainly calculate the correlation amongst predictors and use that to highlight potential problems. For instance, we can calculate the correlation between the predictors easily in `R` using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a0aa6fa",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               wt   wt.copy        hp       cyl\n",
      "wt      1.0000000 0.9734991 0.6587479 0.7824958\n",
      "wt.copy 0.9734991 1.0000000 0.6101981 0.7534608\n",
      "hp      0.6587479 0.6101981 1.0000000 0.8324475\n",
      "cyl     0.7824958 0.7534608 0.8324475 1.0000000\n"
     ]
    }
   ],
   "source": [
    "set.seed(666)\n",
    "data(mtcars)\n",
    "wt      <- mtcars$wt\n",
    "wt.copy <- wt + rnorm(n=length(wt), mean=0, sd=0.2)\n",
    "preds   <- cbind(mtcars,wt.copy)[,c('wt','wt.copy','hp','cyl')] # just the predictors\n",
    "\n",
    "print(cor(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e77478",
   "metadata": {},
   "source": [
    "A threshold of $r > 0.8$ is sometimes used for this purpose, though lower threshold of $r > 0.7$ or $r > 0.6$ are sometimes seen. Depending on the choice, there is either a concern only for `wt.copy`, or a concern for *all* the predictors in this model. This indicates that there is a strong degree of interdependency between all these measurements, which should give us pause to consider whether any of our predictors are redundant or even whether our questions about these data are sensible. In addition, this highlight an issue with raw correlation because this is only an indictor that there *might* be a problem. For the purpose of characterising whether there actually *is* a problem, a better metric is the *Variance Inflation Factor* (VIF).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed6c529",
   "metadata": {},
   "source": [
    "Remembering back to when we defined the sampling distribution of the coefficients in a multiple regression model, we stated that\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{j} \\sim \\mathcal{N}\\left(\\beta_{j},\\frac{\\sigma^{2}}{\\sum{\\left(x_{ij} - \\bar{x}_{j}\\right)^{2} \\times \\left(1 - R^{2}_{j}\\right)}}\\right).\n",
    "$$\n",
    "\n",
    "From this, we can see that the variance is\n",
    "\n",
    "$$\n",
    "\\text{Var}\\left(\\hat{\\beta}_{j}\\right) = \\frac{\\sigma^{2}}{\\sum{\\left(x_{ij} - \\bar{x}_{j}\\right)^{2} \\times \\left(1 - R^{2}_{j}\\right)}}\n",
    "$$\n",
    "\n",
    "which we can rewrite as\n",
    "\n",
    "$$\n",
    "\\text{Var}\\left(\\hat{\\beta}_{j}\\right) = \\frac{\\sigma^{2}}{\\sum{\\left(x_{ij} - \\bar{x}_{j}\\right)^{2}}} \\times \\frac{1}{1 - R^{2}_{j}}\n",
    "$$\n",
    "\n",
    "So the variance can be taken as the expression we already know from simple regression, scaled by the factor $\\frac{1}{1 - R^{2}}$. This is precisely the *variance inflation factor*. So\n",
    "\n",
    "$$\n",
    "\\text{VIF}_{j} = \\frac{1}{1 - R^{2}_{j}}\n",
    "$$\n",
    "\n",
    "and the variance of our parameter estimate can be expressed as\n",
    "\n",
    "$$\n",
    "\\text{Var}\\left(\\hat{\\beta}_{j}\\right) = \\frac{\\sigma^{2}}{\\sum{\\left(x_{ij} - \\bar{x}_{j}\\right)^{2}}} \\times \\text{VIF}_{j}\n",
    "$$\n",
    "\n",
    "The term $R^{2}_{j}$ is something we will cover in more detail *next week* when we discuss model comparisons. For the moment, this can be understood as the proportion of total variation in predictor $j$ that can be explained by all the other predictors in the model. So, when predictors are independent, $R^{2}_{j} = 0$ and $\\text{VIF}_{j} = \\frac{1}{1 - 0} = 1$. This means that there is *no scaling* and the variance of $\\hat{\\beta}_{j}$ is the same as in simple regression. However, for non-independent predictors with (for example) $R^{2}_{j} = 0.9$, we would have $\\text{VIF}_{j} = \\frac{1}{1 - 0.9} = \\frac{1}{0.1} = 10$. This means that the variance of $\\hat{\\beta}_{j}$ is *ten times larger* than it would be if there were independence. \n",
    "\n",
    "In terms of calculating the VIF, we can do this easily using the `vif()` function from the `car` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba452871",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: carData\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       wt   wt.copy        hp       cyl \n",
      "22.174855 19.921678  3.383464  4.794902 \n"
     ]
    }
   ],
   "source": [
    "library(car)\n",
    "mod.multicol <- lm(mpg ~ wt + wt.copy + hp + cyl, data=mtcars)\n",
    "print(vif(mod.multicol))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58debbad",
   "metadata": {},
   "source": [
    "In terms of interpretation, the table below gives some general guidance.\n",
    "\n",
    "| VIF Value     | Multicollinearity  | Interpretation                                                                     |\n",
    "| ------------- | ------------------ | ---------------------------------------------------------------------------------- |\n",
    "| 1             | None               | No issues and no action required.                                                  |\n",
    "| 1 to 5        | Small to Moderate  | Some caution needed, but not usually a problem. Concern gets greater closer to 5.  |\n",
    "| > 5           | High               | Standard errors will be larger than usual and inference will become unstable.      |\n",
    "| > 10          | Severe             | Standard errors will have blown-up and inference will become untrustworthy.        |\n",
    "\n",
    "In the example above, both `wt` and `wt.copy` have VIF > 10, showing *severe* multicollinearity. This points to a big problem with the model, specifically in relation to those two predictors. Notice that `hp` has 1 < VIF < 5, which gives little reason for concern. For `cyl`, that value is also 1 < VIF < 5, however it is much closer to 5 and this may be slightly more concerning. In this situation, we would likely remove either `wt` or `wt.copy`. If we do so and then recalculate VIF, we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c6905c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      wt       hp      cyl \n",
      "2.580486 3.258481 4.757456 \n"
     ]
    }
   ],
   "source": [
    "mod <- lm(mpg ~ wt + hp + cyl, data=mtcars)\n",
    "print(vif(mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103e1a2f",
   "metadata": {},
   "source": [
    "So there is now no immediate cause for concern. The VIF for `cyl` is still getting close to 5, which is something we may need to bear in mind. This is likely due to the fact that engines that produce more horsepower often have more cylinders, which will also have an impact on the weight of the car. So this dependence is somewhat unavoidable, but could cause us to question whether `cyl` is adding anything useful to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf96efc",
   "metadata": {},
   "source": [
    "[^NASA-foot]: [Faraway (2005)](https://www.utstat.toronto.edu/~brunner/books/LinearModelsWithR.pdf) provides a real-world example of why this is *not* good practise. This concerns the delay in the discovery of the hole in the Ozone layer due to NASA's automatic data analysis algorithms discarding very low readings assumed to be mistakes.\n",
    "\n",
    "[^hatmat-foot]: This function is named after the fact that the leverage values come from the diagonal a special matrix called the *hat matrix*, which is used to form the predicted values (i.e. it puts the \"hats\" on $y$ to form $\\hat{y}$).\n",
    "\n",
    "[^levcutoff-foot]: You can verify this for yourself by also calculating `2 * mean(lev)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f56329c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}