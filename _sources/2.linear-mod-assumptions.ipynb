{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6b3fd6-1753-45e7-a920-fccf68cbdcac",
   "metadata": {},
   "source": [
    "# Linear Model Assumptions\n",
    "... In addition, even though we are framing these assumptions in relation to muliple regression, these assumptions are also directly relevant to $t$-tests, ANOVA and ANCOVA models. Part of the utility of the linear models framewowrk is that we can use the same diagnostics to test the same assumptions across any model, meaning we only need to learn *one* set of assumptions and can apply them to any model we choose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7861771",
   "metadata": {},
   "source": [
    "## The Simple Regression Assumptions\n",
    "Before moving on, it will be useful to take a step back and examine all the *assumptions* that are inherent in the model we have specified. This is quite an important exericse, because the assumptions of regression often seem somewhat esoteric when taught outside the framework of statistical models. However, we should be able to derive all of them by simply examining the model equations given above. This is the same approach we will take when we also examine models such as ANOVA and ANCOVA, which will allow us to see that the assumptions *are always the same* when working with these forms of linear models. This means you only need to learn one set of assumptions and they can be applied equivalently across lots of different models. We look at this in more detail next week, so for now we will just have a brief discussion of each.\n",
    "\n",
    "To make this discussion easier, we will restate the most common form of the simple regression model:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    y_{i}   &= \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i} \\\\\n",
    "    \\epsilon_{i} &\\sim \\mathcal{N}\\left(0,\\sigma^{2}\\right),\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eefb4a",
   "metadata": {},
   "source": [
    "### (1) The Outcome is a Continuous Random Variable\n",
    "The first core assumption that we made is that the outcome variable is a continuous random variable. As such, the *data-generating process* can be captured by a continuous probability distribution. This means our outcome variables must, in theory, have an infinite number of possibilities within a given range. Refer back to the lesson last week for more details on this, as well as some of the subtlties around psychologists using discrete measures and treating them as continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12c813c",
   "metadata": {},
   "source": [
    "### (2) The Distribution of the Errors is Normal\n",
    "Following from the first assumption, we have chosen the normal distribution as the population distribution. This can be stated in two ways. The first is as a *marginal* distribution of $y$, given a specific value of $x$. This is expressed as:\n",
    "\n",
    "$$\n",
    "y_{i} \\sim \\mathcal{N}\\left(\\beta_{0} + \\beta_{1}x_{i},\\sigma^{2}\\right)\n",
    "$$\n",
    "\n",
    "This assumption is important to understand, not least because many people *get it wrong*. As should be clear from the equation above, the simple regression model does *not* assume that the entire outcome variable $y$ is normally distributed. Instead, the assumption is that the distribution of $y$ *at each value of $x$* is normally distributed. Considering only the distribution of a *subset* of $y$ values is known as the *marginal* distribution of $y$ given certain values of $x$. However, as we will see next week, this is difficult to work with practically. As such, it is much easier to work with the second form that this assumption takes, which is that the *errors* are *identically distributed* with mean 0, expressed as\n",
    "\n",
    "$$\n",
    "\\epsilon_{i} \\sim \\mathcal{N}\\left(0,\\sigma^{2}\\right).\n",
    "$$\n",
    "\n",
    "Every error is therefore drawn from *the same distribution*. In other words, the errors represents a *sample* from a single distribution. Because of this, we can collapse all the errors together and just examine a *single distribution*. Again, refer back to the content from last week if you need a reminder on why this assumption can be stated in two different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b535c",
   "metadata": {},
   "source": [
    "### (3) The Data/Errors are Uncorrelated\n",
    "... Typically, this is not actually an assumption we test, rather we infer it based on the context of the data collection. For instance, in the `mtcars` example, there is no reason to think that the measurement of `mpg` from one car would impact the measurement from any other car. We therefore assume that all cars are independent and there will be no correlation. However, if we made the same measurement multiple times *on the same car*, then we would probably assume some degree of correlation (this would be an example of *repeated measurements*). Similarly, if we made these measurements over certain time-intervals, then the data would represent a *time-series*, which then would have a very specific correlation structure where values closer in time are more similar than those measured further away. The same is true of working with human subjects. When each measurement represents a different subject, we will typically assume independence. However, multiple measures of the same subject imply some degree of correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673d1c85",
   "metadata": {},
   "source": [
    "\n",
    "`````{admonition} The Complexity of Repeated Measurements\n",
    ":class: tip\n",
    "As indicated above, the typical linear regression model is *not* suitable for repeated measurements. However, these forms of experimental design are so common in Experimental Psychology that they are usually introduced very early on, without much appreciation for the complexity they add to the model. Because of this, everything we cover in this unit will only be applicable to *independent measures* deisgns (also known as *between-subjects* experiments). We will cover how to deal with repeated measurements in the context of *mixed-effects models* later on the course. However, we have a lot of ground to cover before we get there.\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b68a4",
   "metadata": {},
   "source": [
    "### (4) The Relationship is a Straight Line\n",
    "Implicit in the mean function we have specified is the assumption that the relationship in the population that generated our data is a *straight line*. This is not something we can every truly know without access to the whole population. However, we need to examine whether a straight line appears *reasonable*, given the data we have available. This involves examining the fit of the line visually. As we will see next week, this can either be done using just the raw data or, more usefully, can be assessed using some special plots using the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdda7bd9",
   "metadata": {},
   "source": [
    "### (5) The variance is constant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e111963",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4121e285",
   "metadata": {},
   "source": [
    "[^greekfoot]: Ifyou are not very comfortable with this, we would recommend spending some time learning the Greek alphabet. Not only will this help you get more used to the symbols being used, but will also help in terms of communication. You will not be able to verbalise anything about a model if you do not know how to pronounce the Greek alphabet. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
