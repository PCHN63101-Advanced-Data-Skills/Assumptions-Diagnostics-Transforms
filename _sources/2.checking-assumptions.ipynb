{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6b3fd6-1753-45e7-a920-fccf68cbdcac",
   "metadata": {},
   "source": [
    "# Checking the Model Assumptions\n",
    "... In addition, even though we are framing these assumptions in relation to muliple regression, these assumptions are also directly relevant to $t$-tests, ANOVA and ANCOVA models. Part of the utility of the linear models framewowrk is that we can use the same diagnostics to test the same assumptions across any model, meaning we only need to learn *one* set of assumptions and can apply them to any model we choose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7861771",
   "metadata": {},
   "source": [
    "## The Simple Regression Assumptions\n",
    "Before moving on, it will be useful to take a step back and examine all the *assumptions* that are inherent in the model we have specified. This is quite an important exericse, because the assumptions of regression often seem somewhat esoteric when taught outside the framework of statistical models. However, we should be able to derive all of them by simply examining the model equations given above. This is the same approach we will take when we also examine models such as ANOVA and ANCOVA, which will allow us to see that the assumptions *are always the same* when working with these forms of linear models. This means you only need to learn one set of assumptions and they can be applied equivalently across lots of different models. We look at this in more detail next week, so for now we will just have a brief discussion of each.\n",
    "\n",
    "To make this discussion easier, we will restate the most common form of the simple regression model:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    y_{i}   &= \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i} \\\\\n",
    "    \\epsilon_{i} &\\sim \\mathcal{N}\\left(0,\\sigma^{2}\\right),\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eefb4a",
   "metadata": {},
   "source": [
    "### (1) The Outcome is a Continuous Random Variable\n",
    "The first core assumption that we made is that the outcome variable is a continuous random variable. As such, the *data-generating process* can be captured by a continuous probability distribution. This means our outcome variables must, in theory, have an infinite number of possibilities within a given range. Refer back to the lesson last week for more details on this, as well as some of the subtlties around psychologists using discrete measures and treating them as continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12c813c",
   "metadata": {},
   "source": [
    "### (2) The Distribution of the Errors is Normal\n",
    "Following from the first assumption, we have chosen the normal distribution as the population distribution. This can be stated in two ways. The first is as a *marginal* distribution of $y$, given a specific value of $x$. This is expressed as:\n",
    "\n",
    "$$\n",
    "y_{i} \\sim \\mathcal{N}\\left(\\beta_{0} + \\beta_{1}x_{i},\\sigma^{2}\\right)\n",
    "$$\n",
    "\n",
    "This assumption is important to understand, not least because many people *get it wrong*. As should be clear from the equation above, the simple regression model does *not* assume that the entire outcome variable $y$ is normally distributed. Instead, the assumption is that the distribution of $y$ *at each value of $x$* is normally distributed. Considering only the distribution of a *subset* of $y$ values is known as the *marginal* distribution of $y$ given certain values of $x$. However, as we will see next week, this is difficult to work with practically. As such, it is much easier to work with the second form that this assumption takes, which is that the *errors* are *identically distributed* with mean 0, expressed as\n",
    "\n",
    "$$\n",
    "\\epsilon_{i} \\sim \\mathcal{N}\\left(0,\\sigma^{2}\\right).\n",
    "$$\n",
    "\n",
    "Every error is therefore drawn from *the same distribution*. In other words, the errors represents a *sample* from a single distribution. Because of this, we can collapse all the errors together and just examine a *single distribution*. Again, refer back to the content from last week if you need a reminder on why this assumption can be stated in two different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b535c",
   "metadata": {},
   "source": [
    "### (3) The Data/Errors are Uncorrelated\n",
    "... Typically, this is not actually an assumption we test, rather we infer it based on the context of the data collection. For instance, in the `mtcars` example, there is no reason to think that the measurement of `mpg` from one car would impact the measurement from any other car. We therefore assume that all cars are independent and there will be no correlation. However, if we made the same measurement multiple times *on the same car*, then we would probably assume some degree of correlation (this would be an example of *repeated measurements*). Similarly, if we made these measurements over certain time-intervals, then the data would represent a *time-series*, which then would have a very specific correlation structure where values closer in time are more similar than those measured further away. The same is true of working with human subjects. When each measurement represents a different subject, we will typically assume independence. However, multiple measures of the same subject imply some degree of correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20e31ff",
   "metadata": {},
   "source": [
    "`````{admonition} i.i.d. Errors\n",
    ":class: tip\n",
    "One of the more important assumptions about bothe the data and errors in a simple regression model is that they are *uncorrelated*. Formally, we can write\n",
    "\n",
    "$$\n",
    "\\text{Corr}\\left(y_{i},y_{i^{\\prime}}\\right) = \\text{Corr}\\left(\\epsilon_{i},\\epsilon_{i^{\\prime}}\\right) = 0.\n",
    "$$\n",
    "\n",
    "Here, the *prime* symbol ($\\prime$) just means \"a different value of $i$\". So $\\text{Corr}\\left(y_{i},y_{i^{\\prime}}\\right)$ just means \"the correlation between two different values of $i$\". So, in words, this is saying that the correlation between two different values of $y$ is the same as the correlation between the same values of $\\epsilon$, which is equal to 0. This effectively tells us that the values of the outcome/errors *are not correlated*.  \n",
    "\n",
    "The reason this is important is because correlation adds quite a lot of complexity to modelling the data. As such, the basic linear models framework assumes there is none in order to make things simpler. Unfortunately, this precludes several different types of data. Generally speaking, we do not test for correlation, rather we assume it is present or not present based on the experimental design. For instance, if each datapoint represents a measurement from a *different* subject (a *between-subjects* design), then it is reasonable to assume that the data will not be correlated. However, if each datapoint represents a measurement from *the same* subject (a *within-subjects* design), then it is reasonable to assume that data from the same subject *will* be correlated. As such, if we are assuming no correlation then the linear model is not suitable for *repeated measurements* or *longitudinal* data. We will see how to cope with these forms data later in the course when we get to *mixed-effects* models. But, for the time being, we will only be working with data assumed to be *independent*.\n",
    "\n",
    "So, the linear model assumes that both the raw data and the errors are *independant*. However, another useful element of the errors is that they are *identically distributed*. Which is to say that each error is conceptualised as drawn from the same distribution with the same mean and the same variance. This is not true of the outcome variable, because the mean depends upon the values of the predictors. We will see the utility of this next week when we cover diagnostic measures for the linear model. However, for the moment, it is worth noting that the errors are therefore assumed to be *independent* and *identically distributed*, which is a condition often shortened to i.i.d. and expressed using the notation\n",
    "\n",
    "$$\n",
    "\\epsilon_{i} \\overset{i.i.d.}{\\sim} \\mathcal{N}\\left(0,\\sigma^{2}\\right).\n",
    "$$\n",
    "\n",
    "So the i.i.d. assumption for the errors captures the idea that all the errors are random draws from the *same* distribution, and that there is no correlation between them.\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f9e57c",
   "metadata": {},
   "source": [
    "`````{admonition} Residuals are Not Independent with Constant Variance\n",
    ":class: tip\n",
    "One of the main reasons for distinguishing between *errors* and *residuals* is that the estimation process *changes* the distributional properties of the errors. This means that *errors* and *residuals* are not expected to behave idnetically. So while it is correct to assume\n",
    "\n",
    "$$\n",
    "\\epsilon_{i} \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}\\left(0,\\sigma^{2}\\right),\n",
    "$$\n",
    "\n",
    "it is *not* technically correct to assume the same for the *errors*. This is because the estimation procedure can *induce* correlation between the errors and the errors can have non-constant variance, depending upon a property known as *leverage*. We will discuss some of these concepts next week. For now, just note that the residuals can be used as an *approximation* for the errors, but we need to perform some additional checks to make sure that this approximation is reasonable.\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673d1c85",
   "metadata": {},
   "source": [
    "\n",
    "`````{admonition} The Complexity of Repeated Measurements\n",
    ":class: tip\n",
    "As indicated above, the typical linear regression model is *not* suitable for repeated measurements. However, these forms of experimental design are so common in Experimental Psychology that they are usually introduced very early on, without much appreciation for the complexity they add to the model. Because of this, everything we cover in this unit will only be applicable to *independent measures* deisgns (also known as *between-subjects* experiments). We will cover how to deal with repeated measurements in the context of *mixed-effects models* later on the course. However, we have a lot of ground to cover before we get there.\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b68a4",
   "metadata": {},
   "source": [
    "### (4) The Relationship is a Straight Line\n",
    "Implicit in the mean function we have specified is the assumption that the relationship in the population that generated our data is a *straight line*. This is not something we can every truly know without access to the whole population. However, we need to examine whether a straight line appears *reasonable*, given the data we have available. This involves examining the fit of the line visually. As we will see next week, this can either be done using just the raw data or, more usefully, can be assessed using some special plots using the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdda7bd9",
   "metadata": {},
   "source": [
    "### (5) The variance is constant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e111963",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4121e285",
   "metadata": {},
   "source": [
    "[^greekfoot]: Ifyou are not very comfortable with this, we would recommend spending some time learning the Greek alphabet. Not only will this help you get more used to the symbols being used, but will also help in terms of communication. You will not be able to verbalise anything about a model if you do not know how to pronounce the Greek alphabet. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
